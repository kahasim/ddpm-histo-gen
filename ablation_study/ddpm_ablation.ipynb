{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22442,"status":"ok","timestamp":1684092390671,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"MlktmBp9bTRq","outputId":"08893dce-b580-413c-e894-29745cb1d0a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m911.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -U einops datasets matplotlib tqdm pytz icecream\n","\n","import math\n","\n","import copy\n","import os\n","\n","import csv\n","\n","from pathlib import Path\n","from inspect import isfunction\n","from functools import partial\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from plotly.subplots import make_subplots\n","import plotly.graph_objs as go\n","\n","from tqdm.auto import tqdm\n","\n","from datetime import datetime\n","from pytz import timezone\n","\n","from einops import rearrange\n","\n","from scipy.stats import entropy\n","from scipy.linalg import sqrtm\n","\n","from PIL import Image, ImageDraw\n","\n","from IPython.display import HTML\n","\n","import torch\n","from torch import nn, einsum\n","import torch.nn.functional as F\n","from torch.utils.data import random_split, Dataset, DataLoader, Subset\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","import cv2\n","from skimage.metrics import structural_similarity as ssim\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torchvision import utils\n","from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n","from torchvision.utils import save_image\n","\n","from google.colab import files\n","from google.cloud import storage\n","\n","from datasets import load_dataset\n","\n","import pickle\n","\n","import json\n","\n","import glob\n","\n","from icecream import ic\n","\n","%matplotlib inline"],"id":"MlktmBp9bTRq"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20105,"status":"ok","timestamp":1684092410772,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"K10ye-uahWT4","outputId":"66332b40-c638-4441-e8fb-36111780f7bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"K10ye-uahWT4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDNGS2yS9eDS"},"outputs":[],"source":["DOWNLOAD_DATA = False"],"id":"nDNGS2yS9eDS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hz3p4dznbdY9"},"outputs":[],"source":["if DOWNLOAD_DATA:\n","    %cp kaggle.json /content/drive/MyDrive/kaggle\n","    ! mkdir ~/.kaggle\n","    %cp /content/drive/MyDrive/kaggle/kaggle.json ~/.kaggle/\n","    ! chmod 600 ~/.kaggle/kaggle.json\n","    %cd /content/drive/MyDrive/cv-final-project/data\n","    ! kaggle competitions download -c histopathologic-cancer-detection --force\n","    %cd /content"],"id":"Hz3p4dznbdY9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQj6TJLkbfEU"},"outputs":[],"source":["! unzip /content/drive/MyDrive/cv-final-project/data/histopathologic-cancer-detection.zip > /dev/null"],"id":"RQj6TJLkbfEU"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684092526241,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Djh-3NQ-bghC","outputId":"399ab22d-3763-4afb-d4c5-58a23b6a500e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2f83bc88-f288-4483-8283-c476b5edcdec\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>acfe80838488fae3c89bd21ade75be5c34e66be7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7f6ccae485af121e0b6ee733022e226ee6b0c65f</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>559e55a64c9ba828f700e948f6886f4cea919261</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8eaaa7a400aa79d36c2440a4aa101cc14256cda4</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f83bc88-f288-4483-8283-c476b5edcdec')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2f83bc88-f288-4483-8283-c476b5edcdec button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2f83bc88-f288-4483-8283-c476b5edcdec');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                         id  label\n","0  f38a6374c348f90b587e046aac6079959adf3835      0\n","1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n","2  755db6279dae599ebb4d39a9123cce439965282d      0\n","3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n","4  068aba587a4950175d04c680d38943fd488d6a9d      0\n","5  acfe80838488fae3c89bd21ade75be5c34e66be7      0\n","6  a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n","7  7f6ccae485af121e0b6ee733022e226ee6b0c65f      1\n","8  559e55a64c9ba828f700e948f6886f4cea919261      0\n","9  8eaaa7a400aa79d36c2440a4aa101cc14256cda4      0"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["labels_df = pd.read_csv('train_labels.csv')\n","labels_df.head(10)"],"id":"Djh-3NQ-bghC"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1684092526241,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"_T1vZUcdbriS","outputId":"91ab49a4-fc78-478c-c8c7-64b3f02b4211"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-76bfd44f-1cb2-474b-a4e8-82e9943891d5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7f6ccae485af121e0b6ee733022e226ee6b0c65f</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>c3d660212bf2a11c994e0eadff13770a9927b731</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>94fa32b29cc1c00403176c0795fffa3cfaa0f20e</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0b820b71670c039dd0a51333d1c919f471a9e940</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>d34af1e7500f2f3de41b0e6fdeb2ed245d814590</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>6961bdcc16f6c1d7db88fc6a7823178288c2a29e</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>233bf46a575c1731821073e318c029e5df8b12ff</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76bfd44f-1cb2-474b-a4e8-82e9943891d5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-76bfd44f-1cb2-474b-a4e8-82e9943891d5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-76bfd44f-1cb2-474b-a4e8-82e9943891d5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                          id  label\n","1   c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n","6   a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n","7   7f6ccae485af121e0b6ee733022e226ee6b0c65f      1\n","11  c3d660212bf2a11c994e0eadff13770a9927b731      1\n","14  94fa32b29cc1c00403176c0795fffa3cfaa0f20e      1\n","17  0b820b71670c039dd0a51333d1c919f471a9e940      1\n","19  d34af1e7500f2f3de41b0e6fdeb2ed245d814590      1\n","23  464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09      1\n","24  6961bdcc16f6c1d7db88fc6a7823178288c2a29e      1\n","28  233bf46a575c1731821073e318c029e5df8b12ff      1"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["malignant_labels_df = labels_df.loc[labels_df['label'] == 1]\n","malignant_labels_df.head(10)"],"id":"_T1vZUcdbriS"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1684092526241,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"8xTj5iiUbr-g","outputId":"bcfc04d9-0177-4658-c7e8-9b081b8895e1"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2cb53677-2630-4fbd-95e5-d9ac08f1cf39\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>acfe80838488fae3c89bd21ade75be5c34e66be7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>559e55a64c9ba828f700e948f6886f4cea919261</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>8eaaa7a400aa79d36c2440a4aa101cc14256cda4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>a106469bbfda4cdc5a9da7ac0152927bf1b4a92d</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>a1991e73a9b676faddd2bd47c39754b14d1eb923</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>08566ce82d4406f464c9c2a3cd014704735db7a9</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cb53677-2630-4fbd-95e5-d9ac08f1cf39')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2cb53677-2630-4fbd-95e5-d9ac08f1cf39 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2cb53677-2630-4fbd-95e5-d9ac08f1cf39');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                          id  label\n","0   f38a6374c348f90b587e046aac6079959adf3835      0\n","2   755db6279dae599ebb4d39a9123cce439965282d      0\n","3   bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n","4   068aba587a4950175d04c680d38943fd488d6a9d      0\n","5   acfe80838488fae3c89bd21ade75be5c34e66be7      0\n","8   559e55a64c9ba828f700e948f6886f4cea919261      0\n","9   8eaaa7a400aa79d36c2440a4aa101cc14256cda4      0\n","10  a106469bbfda4cdc5a9da7ac0152927bf1b4a92d      0\n","12  a1991e73a9b676faddd2bd47c39754b14d1eb923      0\n","13  08566ce82d4406f464c9c2a3cd014704735db7a9      0"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["non_malignant_labels_df = labels_df.loc[labels_df['label'] == 0]\n","non_malignant_labels_df.head(10)"],"id":"8xTj5iiUbr-g"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1684092526242,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"0m4kMHkncWSO","outputId":"99fba591-20dc-462d-fa5f-a949abf1f2cd"},"outputs":[{"data":{"text/plain":["['.config',\n"," 'drive',\n"," 'train',\n"," 'train_labels.csv',\n"," 'sample_submission.csv',\n"," 'test',\n"," 'sample_data']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir('.')"],"id":"0m4kMHkncWSO"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1684092526242,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Af_29guLcX7t","outputId":"6418f832-0ad8-477a-b1ca-c5ce879e134a"},"outputs":[{"data":{"text/plain":["(220025, 2)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["labels_df.shape"],"id":"Af_29guLcX7t"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684092526242,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"7CPmZzehcZM4","outputId":"a0bbed86-dfa4-442f-9d1f-23964cadcde8"},"outputs":[{"data":{"text/plain":["(89117, 2)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["malignant_labels_df.shape"],"id":"7CPmZzehcZM4"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684092526242,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"OpQgTo_CccGV","outputId":"78737b5b-9db7-432a-a456-beb094d500f0"},"outputs":[{"data":{"text/plain":["(130908, 2)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["non_malignant_labels_df.shape"],"id":"OpQgTo_CccGV"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1684092526633,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Dbxln8mlccbG","outputId":"65b3cb68-744c-4a6d-b07b-7be6dce3129c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-f2e04541-aeb0-4af0-9442-d6ae9deb0a0e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2e04541-aeb0-4af0-9442-d6ae9deb0a0e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f2e04541-aeb0-4af0-9442-d6ae9deb0a0e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f2e04541-aeb0-4af0-9442-d6ae9deb0a0e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["Empty DataFrame\n","Columns: [id, label]\n","Index: []"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["labels_df[labels_df.duplicated(keep=False)]"],"id":"Dbxln8mlccbG"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684092526633,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Yd7tV08oceAP","outputId":"e0aca7d6-088c-406d-9ec0-36f64febd2dd"},"outputs":[{"data":{"text/plain":["0    130908\n","1     89117\n","Name: label, dtype: int64"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["labels_df['label'].value_counts()"],"id":"Yd7tV08oceAP"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1684092526948,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"WnK4jTY4ckqg","outputId":"65f80c45-2d61-46f2-c9c1-0be00c474a22"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/train\n","220025\n","/content\n"]}],"source":["%cd train\n","! ls | wc -l\n","%cd .."],"id":"WnK4jTY4ckqg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5nC7PWmudA-t"},"outputs":[],"source":["img_path = './train'\n","\n","malignant = labels_df.loc[labels_df['label'] == 1]['id'].values\n","non_malignant = labels_df.loc[labels_df['label'] == 0]['id'].values"],"id":"5nC7PWmudA-t"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684092526949,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"OQv2bqBLdCzG","outputId":"324dde5f-7374-46f3-83e3-383b022bbdea"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 10 Non-Malignant IDs\n","========================================\n","f38a6374c348f90b587e046aac6079959adf3835\n","755db6279dae599ebb4d39a9123cce439965282d\n","bc3f0c64fb968ff4a8bd33af6971ecae77c75e08\n","068aba587a4950175d04c680d38943fd488d6a9d\n","acfe80838488fae3c89bd21ade75be5c34e66be7\n","559e55a64c9ba828f700e948f6886f4cea919261\n","8eaaa7a400aa79d36c2440a4aa101cc14256cda4\n","a106469bbfda4cdc5a9da7ac0152927bf1b4a92d\n","a1991e73a9b676faddd2bd47c39754b14d1eb923\n","08566ce82d4406f464c9c2a3cd014704735db7a9\n"]}],"source":["print('First 10 Non-Malignant IDs')\n","print('='*40)\n","for x in range(10): print(non_malignant[x])"],"id":"OQv2bqBLdCzG"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1684092526949,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"Cm6C0zjwdGF-","outputId":"a251046c-e711-4cfc-8bbf-7c6a929d0c7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["First 10 Malignant IDs\n","========================================\n","c18f2d887b7ae4f6742ee445113fa1aef383ed77\n","a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da\n","7f6ccae485af121e0b6ee733022e226ee6b0c65f\n","c3d660212bf2a11c994e0eadff13770a9927b731\n","94fa32b29cc1c00403176c0795fffa3cfaa0f20e\n","0b820b71670c039dd0a51333d1c919f471a9e940\n","d34af1e7500f2f3de41b0e6fdeb2ed245d814590\n","464327050ef07bb927f8bfb5c4e4dd5ebd4d3c09\n","6961bdcc16f6c1d7db88fc6a7823178288c2a29e\n","233bf46a575c1731821073e318c029e5df8b12ff\n"]}],"source":["print('First 10 Malignant IDs')\n","print('='*40)\n","for x in range(10): print(malignant[x])"],"id":"Cm6C0zjwdGF-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oIdHcgd9dH2R"},"outputs":[],"source":["def plot_fig(ids:str=None, title:str=None, nrows:int=5, ncols:int=15):\n","    \n","    fig, ax = plt.subplots(nrows, ncols, figsize=(18,6))\n","    plt.subplots_adjust(wspace=0, hspace=0)\n","\n","    for i,j in enumerate(ids[:nrows*ncols]):\n","        file_name = os.path.join(img_path, j + '.tif')\n","        img = Image.open(file_name)\n","\n","        id_col = ImageDraw.Draw(img)\n","        id_col.rectangle(((0,0), (95,95)), outline='white')\n","\n","        plt.subplot(nrows, ncols, i+1)\n","        plt.imshow(np.array(img))\n","        plt.axis('off')\n","\n","    plt.suptitle(title, y=0.92)"],"id":"oIdHcgd9dH2R"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488,"output_embedded_package_id":"1W49WAZRH0NB5jL8hqvu49nqUwQw3Fx2m"},"executionInfo":{"elapsed":13797,"status":"ok","timestamp":1684092540744,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"VCnu3A54dKUp","outputId":"6d3704e2-6c4c-4c5f-a9b0-43dc61e1bb85"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["plot_fig(malignant, 'Malignant Cases')"],"id":"VCnu3A54dKUp"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488,"output_embedded_package_id":"1j_UfdrZ-Dk3QLCHb9Wjo5qlF16zF6tQS"},"executionInfo":{"elapsed":10870,"status":"ok","timestamp":1684092551605,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"21eSMRjudKST","outputId":"af9b1aef-bd5b-4cdc-deee-9c20d76d3d8f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["plot_fig(non_malignant, 'Non-Malignant Cases')"],"id":"21eSMRjudKST"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1684092551605,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"BXJtDyWjdKPq","outputId":"09bd2f56-f161-4f47-e456-a108fe2aae4b"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f0c278c5ed0>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(12664675) # We create a manual seed"],"id":"BXJtDyWjdKPq"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAN9PO55dKNk"},"outputs":[],"source":["class HistoDataset(Dataset):\n","\n","    def __init__(self, data_dir, data_type='train', data_partition=None, split=None, seed=None, num_samples=None, **kwargs):\n","        ## Get image file names\n","        data_type_dir = os.path.join(data_dir, data_type) ## \"Data dir\" simply acts as a placeholder for the current directory, \"data_type\" defaults to \"train\", given it states the folder of avaialble data, and we do not have labels distincting the data in \"test\"\n","\n","        if 'is_malignant' in kwargs:\n","            self.is_malignant = kwargs['is_malignant']\n","        else:\n","            # Default self.is_malignant to True\n","            self.is_malignant = True\n","        \n","        ## Change the file names based on whether we are asking for a malignant or non-malignant dataset\n","        self.labels_df = malignant_labels_df['id'] if self.is_malignant else non_malignant_labels_df['id']\n","        self.file_names = self.labels_df.values  \n","\n","        ## If 'split' is provided split the data into train/test proportions\n","        if split is not None:\n","            assert 0.0 < split < 1.0, 'Split should be between 0.0 and 1.0'\n","            if seed is not None:\n","                np.random.seed(seed)\n","            np.random.shuffle(self.file_names)\n","            split_idx = int(len(self.file_names) * split)\n","            if data_partition == 'train':\n","                self.file_names = self.file_names[:split_idx]\n","            elif data_partition == 'eval':\n","                self.file_names = self.file_names[split_idx:]\n","            else:\n","                raise ValueError('\\\"data_partition\\\" invalid: should be \\\"train\\\" or \\\"test\\\"')\n","\n","        ## Selects number of samples based on kwargs for number of samples to select, raises error if number of samples is greater than total number of files\n","        if num_samples is not None:\n","            if num_samples == 'all':\n","                pass\n","            elif num_samples > len(self.file_names):\n","                raise ValueError('\\\"num_samples\\\" cannot be larger than the total number of samples')\n","            else:\n","                self.file_names = self.file_names[:num_samples]\n","\n","        ## Continuation of previous block, sets the number of samples as the length of file names (after being chosen by previous block)\n","        self.num_samples = len(self.file_names)\n","\n","        ## Sets the parameterized transformations available to creating the dataset\n","        if 'transforms' in kwargs:\n","            self.transforms = kwargs['transforms']\n","        else:\n","            self.transforms = transforms.Compose([])\n","\n","        ## Code block to return only the center crop 32x32 pixels that were used in the PCam dataset to identify whether to only use the cancer-identified pixels\n","        if 'malignant_crop' in kwargs:\n","            self.malignant_crop = kwargs['malignant_crop']\n","            self.return_malignant_section = True\n","            self.image_size = 32\n","        else:\n","            self.return_malignant_section = False\n","            self.image_size = 96\n","\n","        ## Using kwargs sets whether to return a PIL or torch.tensor representation of the image\n","        if 'return_PIL' in kwargs:\n","            self.return_PIL = kwargs['return_PIL']\n","        else:\n","            self.return_PIL = True\n","\n","        ## Sets number of channels in image, else defaults to 3 (R,G,B)\n","        if 'channels' in kwargs:\n","            self.channels = kwargs['channels']\n","        else:\n","            self.channels = 3\n","\n","        '''\n","        Defines the reverse transforms to be used later on, depending on the number of desired channels and\n","            whether or not a PIL Image is wanting to be returned\n","        '''\n","        if 'reverse_transforms_list' in kwargs:\n","            intermediate_transform = Lambda(lambda t: rearrange(t, 'i j -> j i')) if self.channels == 1 else Lambda(lambda t: rearrange(t, 'i j k -> j k i'))\n","            self.reverse_transforms_list = kwargs['reverse_transforms_list']\n","\n","            if 'reverse_return_PIL' in kwargs:\n","                self.reverse_return_PIL = kwargs['reverse_return_PIL']\n","\n","            for idx in range(len(self.reverse_transforms_list)):\n","                if self.reverse_transforms_list[idx] == 'channels':\n","                    self.reverse_transforms_list[idx] = intermediate_transform\n","\n","                elif self.reverse_transforms_list[idx] == 'reverse_return_PIL':\n","                    self.reverse_transforms_list[idx] = ToPILImage() if self.reverse_return_PIL else Compose([])\n","\n","            self.reverse_transforms = Compose(self.reverse_transforms_list)\n","        else:\n","            self.reverse_transforms = None\n","\n","        ## Choose indices for random samples, else use all samples if self.num_samples == len(self.file_names)\n","        self.files_choose = np.random.choice(self.file_names, \n","                                             self.num_samples, \n","                                             replace=False).tolist()\n","\n","        ## Get all file pathes that make up the selected images in the dataset\n","        self.full_file_path = [os.path.join(data_type_dir, file_name + '.tif') for file_name in self.files_choose]\n","\n","        ## Defines a pytorch.transforms functions to transform from PIL Image to PyTorch Tensor\n","        self.to_tensor = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        # Open image, apply transforms and return with label\n","        image = Image.open(self.full_file_path[idx])  # Open Image with PIL\n","        \n","        if self.channels == 1:\n","            image = image.convert('L')\n","\n","        if self.return_malignant_section:\n","            image = self.malignant_crop(image)\n","\n","        image_tensor = self.transforms(image) # Apply set transformations to Image\n","\n","        if self.return_PIL:\n","            return image\n","        else:\n","            return image_tensor"],"id":"NAN9PO55dKNk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyT8PY6sdKKs"},"outputs":[],"source":["malignant_crop = transforms.CenterCrop((32, 32)) # We only want the innermost, identifiable 32x32px region of tumor tissue"],"id":"fyT8PY6sdKKs"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYacD2-4dKH1"},"outputs":[],"source":["## As stated in the DDPM paper, restricts the values of all pixels between [-1,1]\n","img_transforms = Compose(\n","    [\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda t: (2 * t) - 1)\n","    ]\n",")"],"id":"yYacD2-4dKH1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Uk18ELD9wP0"},"outputs":[],"source":["## numpy implementation for reverse_transforms, moving values: [-1, 1] -> [1, 0]\n","reverse_transforms_list_np = [\n","    Lambda(lambda t: t.transpose(1, 2, 0)),\n","    Lambda(lambda t: (t + 1) / 2),\n","    'channels', ## Will replace programatically based on whether using grayscale or RBG (1 or 3 channels)\n","    Lambda(lambda t: np.clip(t, 0, 1)),\n","    'reverse_return_PIL', ## Will replace programatically based on whether needing to return a PIL Image\n","]"],"id":"4Uk18ELD9wP0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrBGmZNcyKjH"},"outputs":[],"source":["## torch implementation for reverse_transforms, moving values: [-1, 1] -> [1, 0]\n","reverse_transforms_list = [\n","    Lambda(lambda t: t.permute(1, 2, 0)),\n","    Lambda(lambda t: (t + 1) / 2),\n","    'channels', ## Will replace programatically based on whether using grayscale or RBG (1 or 3 channels)\n","    Lambda(lambda t: torch.clamp(t, 0, 1)),\n","    'reverse_return_PIL', ## Will replace programatically based on whether needing to return a PIL Image\n","]"],"id":"yrBGmZNcyKjH"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1684092551607,"user":{"displayName":"Sunny Son","userId":"10863006580902677922"},"user_tz":240},"id":"3a2m-t307UZk","outputId":"5706337e-df17-4350-a2ac-3f751f439315"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["! pwd"],"id":"3a2m-t307UZk"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ewRgMVl7Y2U"},"outputs":[],"source":["data_dir = '.'"],"id":"9ewRgMVl7Y2U"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7uK1XEldKCW"},"outputs":[],"source":["## Instantiate the dataset using malignant/non-malignant samples \n","train_dataset = HistoDataset(data_dir=data_dir,\n","                             data_type='train',\n","                             data_partition='train',\n","                             split=0.9, \n","                             seed=12664675, \n","                             num_samples='all',\n","                             is_malignant=True,\n","                             malignant_crop=malignant_crop,\n","                             transforms=img_transforms,\n","                             reverse_transforms_list=reverse_transforms_list,\n","                             reverse_return_PIL=False,\n","                             return_PIL=True,\n","                             channels=3\n",")"],"id":"Q7uK1XEldKCW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"glEQUGDM7wRF"},"outputs":[],"source":["eval_dataset = HistoDataset(data_dir=data_dir, \n","                            data_type='train',\n","                            data_partition='eval',\n","                            split=0.9, \n","                            seed=12664675, \n","                            num_samples='all', \n","                            is_malignant=True,\n","                            malignant_crop=malignant_crop,\n","                            transforms=img_transforms,\n","                            reverse_transforms_list=reverse_transforms_list,\n","                            reverse_return_PIL=False,\n","                            return_PIL=True,\n","                            channels=3\n",")"],"id":"glEQUGDM7wRF"},{"cell_type":"markdown","metadata":{"id":"cc01c63b"},"source":["The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n","\n","## Define a PyTorch DataLoader\n","\n","Here we define a regular PyTorch DataLoader, as well as creating variables to store information about the dataset (`image_size`, `channels`, and `batch_size`)\n","\n","Finally, we define a `DataLoader` from the `HistoDataset` class created and instantiated previously"],"id":"cc01c63b"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6134d691"},"outputs":[],"source":["image_size = train_dataset.image_size\n","channels = train_dataset.channels\n","batch_size = 64"],"id":"6134d691"},{"cell_type":"markdown","metadata":{"id":"db6f5875"},"source":["Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the $[-1,1]$ range."],"id":"db6f5875"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b3e78945"},"outputs":[],"source":["## Changes the train/eval_dataset instance variable to return a torch.Tensor instead of a PILImage\n","train_dataset.return_PIL = False\n","eval_dataset.return_PIL = False\n","\n","## Create train/eval_loader\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)"],"id":"b3e78945"},{"cell_type":"markdown","metadata":{"id":"6CtJltDJOqKg"},"source":["# UNet Associated Classes\n","\n","---\n","\n","## Base Block"],"id":"6CtJltDJOqKg"},{"cell_type":"code","execution_count":null,"metadata":{"id":"63qmEqb_MVfI"},"outputs":[],"source":["class BaseBlock(nn.Module):\n","    '''\n","    Base block for UNet with time embedding\n","    '''\n","    def __init__(self, in_ch, out_ch, t_dim=512):\n","        super().__init__()\n","        self.t_mlp = nn.Linear(t_dim, in_ch)\n","\n","        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_ch)\n","\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_ch)\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        \n","    def forward(self, x, t):\n","        t_emb = self.t_mlp(t)\n","        t_emb = t_emb.reshape(*t_emb.shape, 1, 1)\n","\n","        x = x + t_emb # time is added (what about concat?)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x"],"id":"63qmEqb_MVfI"},{"cell_type":"markdown","metadata":{"id":"RNzLGvHYOlln"},"source":["## Encoder/Decoder Blocks"],"id":"RNzLGvHYOlln"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaQwrHiYMyCg"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","    '''\n","    UNet encoder\n","    '''\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.base = BaseBlock(in_ch, out_ch)\n","        self.downsample = nn.MaxPool2d(2)\n","        \n","    def forward(self, x, t):\n","        x = self.base(x, t)\n","        x_down = self.downsample(x)\n","\n","        return x_down, x # Return x for residual into Decoder\n","\n","class DecoderBlock(nn.Module):\n","    '''\n","    UNet decoder\n","    '''\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.upsample = nn.ConvTranspose2d(in_ch, out_ch, 2, 2)\n","        self.base = BaseBlock(in_ch, out_ch)\n","        \n","    def forward(self, x, f, t):\n","        x = self.upsample(x)\n","        x = torch.cat([x, f], dim=1) # Concat residual features from Encoder\n","        x = self.base(x, t)\n","\n","        return x"],"id":"uaQwrHiYMyCg"},{"cell_type":"markdown","metadata":{"id":"Sdjlzb-fO2UX"},"source":["## Sinusodial Positional Embedding with (Optional) Self-Attention"],"id":"Sdjlzb-fO2UX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xxKxcgyMyu9"},"outputs":[],"source":["class SinusoidalPositionEmbedding(nn.Module):\n","    '''\n","    Transformer Sinusoidal Position(Time) Embedding in 'Attention is All You Need' paper\n","    '''\n","    def __init__(self, t_dim=512, device='cuda'):\n","        super().__init__()\n","        n = 10000\n","        self.w = 1. / torch.pow(n, 2 * torch.arange(t_dim // 2, device=device) / t_dim)\n","                \n","    def forward(self, t):\n","        t = t[:, None] * self.w[None, :]\n","        sin_t = torch.sin(t)\n","        cos_t = torch.cos(t)\n","        embedding = torch.stack([sin_t, cos_t], dim=-1).flatten(start_dim=1)\n","        return embedding\n","\n","class SelfAttention(nn.Module):\n","    def __init__(self, channels, heads=4):\n","        super(SelfAttention, self).__init__()\n","        self.channels = channels\n","        self.heads = heads\n","        self.mha = nn.MultiheadAttention(channels, heads, batch_first=True)\n","        self.layer_norm = nn.LayerNorm([channels])\n","        self.ff_self = nn.Sequential(\n","            nn.LayerNorm([channels]),\n","            nn.Linear(channels, channels),\n","            nn.GELU(),\n","            nn.Linear(channels, channels),\n","        )\n","\n","    def forward(self, x):\n","        batch_size, channels, height, width = x.shape\n","        x = x.view(batch_size, channels, -1).permute(0, 2, 1)\n","        x_ln = self.layer_norm(x)\n","\n","        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n","        attention_value = attention_value + x\n","        attention_value = self.ff_self(attention_value) + attention_value\n","        attention_value = attention_value.permute(0, 2, 1).view(batch_size, channels, height, width)\n","\n","        return attention_value"],"id":"5xxKxcgyMyu9"},{"cell_type":"markdown","metadata":{"id":"YeDjFRDgO9nG"},"source":["## U-Net Class"],"id":"YeDjFRDgO9nG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0N5fzRLM4_U"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, attention_layers=(2, 4, 6, 8), t_dim=512):\n","        super().__init__()\n","\n","        # Time embedding\n","        self.t_mlp = nn.Sequential(\n","            SinusoidalPositionEmbedding(),\n","            nn.Linear(t_dim, t_dim),\n","        )\n","\n","        # Encoder blocks\n","        self.enc_1 = EncoderBlock(3, 64)\n","        self.sa_enc_1 = SelfAttention(64) if (1 in attention_layers) else nn.Identity()\n","\n","        self.enc_2 = EncoderBlock(64, 128)\n","        self.sa_enc_2 = SelfAttention(128) if (2 in attention_layers) else nn.Identity()\n","\n","        self.enc_3 = EncoderBlock(128, 256)\n","        self.sa_enc_3 = SelfAttention(256) if (3 in attention_layers) else nn.Identity()\n","\n","        self.enc_4 = EncoderBlock(256, 512)\n","        self.sa_enc_4 = SelfAttention(512) if (4 in attention_layers) else nn.Identity()\n","\n","        # Mid block\n","        self.mid_5 = BaseBlock(512, 1024)\n","        self.sa_mid_5 = SelfAttention(1024) if (5 in attention_layers) else nn.Identity()\n","\n","        # Decoder blocks\n","        self.dec_6 = DecoderBlock(1024, 512)\n","        self.sa_dec_6 = SelfAttention(512) if (6 in attention_layers) else nn.Identity()\n","\n","        self.dec_7 = DecoderBlock(512, 256)\n","        self.sa_dec_7 = SelfAttention(256) if (7 in attention_layers) else nn.Identity()\n","\n","        self.dec_8 = DecoderBlock(256, 128)\n","        self.sa_dec_8 = SelfAttention(128) if (8 in attention_layers) else nn.Identity()\n","\n","        self.dec_9 = DecoderBlock(128, 64)\n","        self.sa_dec_9 = SelfAttention(64) if (9 in attention_layers) else nn.Identity()\n","\n","        # Final decode\n","        self.out = nn.Conv2d(64, 3, 1)\n","        \n","    def forward(self, x, t):\n","        t = self.t_mlp(t)\n","        \n","        x, r_1 = self.enc_1(x, t)\n","        x = self.sa_enc_1(x)\n","\n","        x, r_2 = self.enc_2(x, t)\n","        x = self.sa_enc_2(x)\n","\n","        x, r_3 = self.enc_3(x, t)\n","        x = self.sa_enc_3(x)\n","\n","        x, r_4 = self.enc_4(x, t)\n","        x = self.sa_enc_4(x)\n","\n","        x = self.mid_5(x, t)\n","        x = self.sa_mid_5(x)\n","\n","        x = self.dec_6(x, r_4, t)\n","        x = self.sa_dec_6(x)\n","\n","        x = self.dec_7(x, r_3, t)\n","        x = self.sa_dec_7(x)\n","\n","        x = self.dec_8(x, r_2, t)\n","        x = self.sa_dec_8(x)\n","\n","        x = self.dec_9(x, r_1, t)\n","        x = self.sa_dec_9(x)\n","        \n","        x = self.out(x)\n","\n","        return x"],"id":"g0N5fzRLM4_U"},{"cell_type":"markdown","metadata":{"id":"yF9fsEVKcRXU"},"source":["# Diffusion Class\n","\n","---"],"id":"yF9fsEVKcRXU"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tKX9GYfVoxH"},"outputs":[],"source":["class DiffusionModel():\n","    def __init__(self, timesteps=1000, schedule='cosine_beta_schedule', attention_layers=(2, 4, 6, 8), beta_start=1e-4, beta_end=2e-2, s=0.008, image_size=32, channels=3, device='cuda'):\n","        \n","        ## Instantiation attributes/objects from parameters\n","        self.timesteps = timesteps\n","        \n","        self.schedule = schedule\n","        self.attention_layers = attention_layers\n","\n","        self.beta_start = beta_start\n","        self.beta_end = beta_end\n","        self.s = s\n","\n","        self.channels = channels\n","        self.image_size = image_size\n","        self.device = device\n","        \n","        ## Derived attributes/objects\n","        self.beta = self.cosine_beta_schedule(timesteps=self.timesteps, s=self.s).to(self.device) \\\n","                    if self.schedule == 'cosine_beta_schedule' else self.linear_beta_schedule(timesteps=self.timesteps, beta_start=self.beta_start, beta_end=self.beta_end).to(self.device)\n","        \n","        self.alpha = 1. - self.beta\n","        self.alpha_bar = torch.cumprod(self.alpha, axis=0)\n","\n","        self.unet = UNet(attention_layers=self.attention_layers)\n","\n","    def cosine_beta_schedule(self, timesteps, s=0.008):\n","        '''\n","        cosine beta schedule as proposed in https://arxiv.org/abs/2102.09672\n","        '''\n","        total_steps = timesteps + 1\n","        x = torch.linspace(0, timesteps, total_steps)\n","        alpha_bar = torch.pow(torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5), 2)\n","        alpha_bar = alpha_bar / alpha_bar[0]\n","        betas = 1 - (alpha_bar[1:] / alpha_bar[:-1])\n","        return torch.clip(betas, 0.0001, 0.9999)\n","\n","    def linear_beta_schedule(self, timesteps, beta_start=0.0001, beta_end=0.02):\n","        return torch.linspace(beta_start, beta_end, timesteps)\n","\n","    def forward_process(self, x_0, t):\n","        '''\n","        aka diffusion process\n","        '''\n","        eps = torch.randn_like(x_0).to(self.device)\n","        alpha_bar_t = self.alpha_bar[t].reshape(-1, *((1,) * (len(x_0.shape) - 1)))\n","        x_t = torch.sqrt(alpha_bar_t) * x_0 + torch.sqrt(1 - alpha_bar_t) * eps ## Reparameterization trick\n","        return x_t, eps\n","\n","    def loss_function(self, model, x_0, t):\n","        '''\n","        simple MSE loss in DDPM paper\n","        '''\n","        x_t, eps = self.forward_process(x_0, t) ## t used as index\n","        eps_pred = model(x_t, t.clone())\n","        loss = nn.functional.mse_loss(eps, eps_pred)\n","        return loss\n","\n","    def get_unet(self):\n","        return self.unet\n","\n","    def set_unet(self, unet):\n","        self.unet = unet\n","\n","    def reverse_transform(img:np.ndarray):\n","        img = img.transpose(1, 2, 0)\n","        img = (img + 1) / 2\n","        img = np.clip(img, 0, 1)\n","\n","        return img\n","    \n","    def reverse_transform_torch(img):\n","        img = img.permute(1, 2, 0)\n","        img = (img + 1) / 2\n","        img = np.clamp(img, 0, 1)\n","\n","        return img\n","\n","    def show_images(imgs, reverse_process=True):\n","        '''\n","        Function to plot outputs in format of generated images in a list or batches of images in numpy.array or torch.tensor format\n","            @params imgs: list, numpy.array, or torch.tensor of images\n","            @params reverse_process: Reverses the transformed values of [-1, 1] back to [0, 1], as accepted by imshow and other plotting packages\n","        '''\n","        nums = len(imgs) if isinstance(imgs, list) else 1 if len(imgs.shape) == 3 else imgs.shape[0] # list or batch (of numpy or torch) of single or multiple images\n","        num_rows = (nums + 9) // 10\n","        num_cols = min(nums, 10)\n","\n","        fig = plt.figure(figsize=(num_cols, num_rows))\n","        for idx in range(nums):\n","            img = imgs[idx] if nums > 1 else imgs\n","\n","            if reverse_process:\n","                if isinstance(imgs[idx], torch.Tensor):\n","                    img.detach().cpu().numpy().transpose(1, 2, 0)\n","                    img = (img + 1) / 2\n","                    img = np.clip(img, 0, 1)\n","                else:\n","                    img = img.transpose(1, 2, 0)\n","                    img = (img + 1) / 2 ## [-1, 1] --> [0, 1]\n","                    img = np.clip(img, 0, 1) ## might be out of [0, 1] due to added noise\n","\n","            ax = plt.subplot(num_rows, num_cols, idx + 1)\n","            plt.imshow(img)\n","            plt.axis('off')\n","        plt.subplots_adjust(wspace=0.1, hspace=0.1)\n","        plt.show()\n","\n","    @torch.no_grad()\n","    def sample_image(model, beta, alpha, alpha_bar, timesteps, img_size, device='cuda'):\n","        '''\n","        Implementation of Algorithm 2 Sampling, for the same model across a single batch, across all timesteps\n","            @param model: Model of UNet Model class defined above (implementing nn.Module)\n","            @param beta: beta values from DiffusionModel class\n","            @param alpha: alpha values from DiffusionModel class\n","            @param alpha_bar: alpha_bar values from DiffusionModel class\n","            @param timesteps: int timesteps to show how many timesteps diffusion takes place across\n","            @param img_size: int size of image height (= width)\n","            @param device: Specifying which device to run processes on, defaults to 'cuda'\n","            @returns x_p: For a single image processed, a torch.tensor of shape (timesteps x channels x height x width)\n","            @returns x_img: Returns the last timestep of x_p, or a torch.tensor of shape (channels x height x width)\n","        '''\n","\n","        x = torch.randn((1, channels, img_size, img_size), device=device)\n","        x_p = torch.empty((timesteps, channels, img_size, img_size))\n","\n","        for t in tqdm(reversed(range(0, timesteps)), desc='Sampling timestep loop', total=timesteps):\n","            z = torch.randn_like(x) if t > 1 else torch.zeros_like(x)\n","            z = z.to(device)\n","\n","            t_tensor = torch.tensor(t, dtype=torch.float32).unsqueeze(0).to(device)\n","            eps_pred = model(x, t_tensor)\n","\n","            x_pred = 1 / torch.sqrt(alpha[t]) * (x - beta[t] / torch.sqrt(1 - alpha_bar[t]) * eps_pred) + torch.sqrt(beta[t]) * z\n","            x = x_pred ## p(x_{t-1}|x_t)\n","            x_p[-t] = x.cpu()\n","            x_img = x_p[-1]\n","\n","        return x_p, x_img\n","\n","    @torch.no_grad()\n","    def sample_batch(diffusion_model, batch_size=32, device='cuda', return_torch=False):\n","        '''\n","        Implementation of batched version for Algorithm 2 Sampling for the same model, across all batches.\n","            @param diffusion_model: Intaking an object of the DiffusionModel class\n","            @param batch_size: Taking an int specifying the size per batch to process\n","            @param device: Specifying which device to run processes on, defaults to 'cuda'\n","            @return x_b: Batched (tensor) version of images processed across ALL timesteps\n","            @return x_b.numpy(): Batched numpy array version of images processed across ALL timesteps\n","            @return x_b_img: Batched torch single image\n","            @return x_b_img.numpy(): Batched numpy single image\n","        '''\n","\n","        model = diffusion_model.get_unet()\n","        model.eval()\n","\n","        beta = diffusion_model.beta\n","        alpha =  diffusion_model.alpha\n","        alpha_bar = diffusion_model.alpha_bar\n","\n","        timesteps = diffusion_model.timesteps\n","        img_size = diffusion_model.image_size\n","        channels = diffusion_model.channels\n","\n","        x_b = torch.empty((batch_size, timesteps, channels, img_size, img_size))\n","        x_b_img = torch.empty((batch_size, channels, img_size, img_size))\n","\n","        for idx in tqdm(range(batch_size), desc='Sampling batch loop', total=batch_size):\n","            x_p, x_img = sample_image(model=model, beta=beta, alpha=alpha, img_size=img_size, alpha_bar=alpha_bar, timesteps=timesteps)\n","\n","            x_b[idx] = x_p\n","            x_b_img[idx] = x_img\n","\n","        if return_torch:\n","            return x_b, x_b_img\n","        else:\n","            return x_b.numpy(), x_b_img.numpy()"],"id":"8tKX9GYfVoxH"},{"cell_type":"markdown","metadata":{"id":"22e4c0fd"},"source":["# Train the Model\n","\n","---\n","\n","Next, we train the model in regular PyTorch fashion.\n","\n","Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).\n","\n","The environment variables\n","\n","```unix\n","TRAIN_MODEL\n","EVAL_MODEL\n","SAMPLE_MODEL\n","```\n","\n","Each:\n","\n","> Deciding whether to train the model\n","\n","> Whether to evaluate the model on metrics to be shown later\n","\n","> Whether to sample images from the trained model\n","\n","Set whether to train a new `UNet` model or evaluate the models using metrics to be defined later on"],"id":"22e4c0fd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"m00CbLWYAfWe"},"outputs":[],"source":["TRAIN_MODEL = False\n","EVAL_MODEL = False\n","SAMPLE_MODEL = False"],"id":"m00CbLWYAfWe"},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9Bjf8XnBeMO"},"outputs":[],"source":["def save_diffusion(diffusion_model:DiffusionModel, save_str:str, save_to_drive:bool, schedule, attention):\n","\n","    if not save_to_drive:\n","        models_folder = Path('./models')\n","        models_folder.mkdir(exist_ok=True)\n","\n","        model_save_path = str(models_folder / save_str) + '.pt'\n","\n","    else:\n","        model_save_path = f'./drive/MyDrive/cv-final-project/ablation_study/ablation_trained/{schedule}/{attention}/' + save_str + '.pt'\n","\n","    torch.save(diffusion_model, model_save_path)"],"id":"S9Bjf8XnBeMO"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-m-tvGjnetM"},"outputs":[],"source":["def save_loss(loss_list, save_str:str, save_to_drive:bool, schedule, attention):\n","    \n","    if not save_to_drive:\n","        loss_list_folder = Path('./loss_list')\n","        loss_list_folder.mkdir(exist_ok=True)\n","\n","        loss_list_save_path = str(loss_list_folder / save_str) + '.csv'\n","\n","    else:\n","        loss_list_save_path = f'./drive/MyDrive/cv-final-project/ablation_study/ablation_trained/{schedule}/{attention}/' + save_str + '_loss_list.csv'\n","\n","    loss_df = pd.DataFrame(loss_list)\n","    loss_df.to_csv(loss_list_save_path, header=False)"],"id":"f-m-tvGjnetM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBR2MwmKyq_p"},"outputs":[],"source":["def train_model(epochs:int=50,\n","                print_every_n_steps:int=100,\n","                save_diffusion_every_n_epochs:int=10,\n","                save_loss_every_n_epochs:int=1,\n","                schedule:str='linear_beta_schedule',\n","                attention:str='self_attention'\n","):\n","\n","    ## Instantiate the diffusion class/model for whether to use any self-attention layers\n","    attention_layers = (2, 4, 6, 8) if attention == 'self_attention' else ();\n","    diffusion = DiffusionModel(attention_layers=attention_layers)\n","    model = diffusion.get_unet()\n","    model.train()\n","\n","    ## Instantiates device, moves model to device. and sets optimizer to Adam\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    model.to(device)\n","    optimizer = Adam(model.parameters(), lr=1e-5)\n","\n","    model_channel_str = 'RGB_model_' if diffusion.channels == 3 else 'gray_model_'\n","\n","    loss_list = []\n","    model.train()\n","    for epoch in range(1, epochs+1):\n","        for step, batch in enumerate(train_loader, 1):\n","            optimizer.zero_grad() ## Zeroes out optimizer\n","\n","            batch_size = batch.shape[0] ## Defines batch_size as the first index of the batch.shape\n","            batch = batch.to(device) ## Sends batch to device\n","\n","            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long() ## Algorithm 1 line 3: sample t uniformally for every example in the batch\n","\n","            loss = diffusion.loss_function(model, batch, t)\n","            loss_value = loss.item() / len(train_loader)\n","\n","            if step % print_every_n_steps == 0 or step == 1: ## Prints out the first loss and every loss for previously determined step size\n","                print(f'Epoch: {epoch} | Step: {step} | Image: {step * batch_size}/{len(train_dataset)} | Model Type: [\\\"{schedule}\\\"; \\\"{attention}\\\"] | Loss: {loss_value}')\n","\n","            if epoch % save_diffusion_every_n_epochs == 0 and step == 1: ## Saves the diffusion model every n epochs, while also fulfilling that the number of steps be 0\n","                save_str = model_channel_str + f'{diffusion.image_size}px_{epoch}epoch_{schedule}_{attention}'\n","                save_diffusion(diffusion_model=diffusion, save_str=save_str, save_to_drive=True, schedule=schedule, attention=attention)\n","\n","            if step == 1: ## Adds to the loss list (used for graphing later on) at the beginning of every step\n","                loss_list.append(loss_value)\n","\n","                if epoch % save_loss_every_n_epochs == 0: ## Saves the loss list as a CSV file every n epochs\n","                    save_str = model_channel_str + f'{diffusion.image_size}px_{epoch}epoch_{schedule}_{attention}'\n","                    save_loss(loss_list=loss_list, save_str=save_str, save_to_drive=True, schedule=schedule, attention=attention)\n","\n","            loss.backward()\n","            optimizer.step()"],"id":"yBR2MwmKyq_p"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1iXXmMuFsvw"},"outputs":[],"source":["schedule_list = ['linear_beta_schedule', 'cosine_beta_schedule']\n","attention_list = ['no_attention', 'self_attention']"],"id":"Q1iXXmMuFsvw"},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlGCTlYXkmve"},"outputs":[],"source":["train_epochs = 100"],"id":"jlGCTlYXkmve"},{"cell_type":"code","execution_count":null,"metadata":{"id":"uB6xCcy267IV"},"outputs":[],"source":["if TRAIN_MODEL:\n","    full_str_list = []\n","    for schedule in schedule_list:\n","        for attention in attention_list:\n","\n","            train_model(epochs=train_epochs, schedule=schedule, attention=attention)\n","            full_str_list.append((schedule, attention))\n","\n","            path_str = './drive/MyDrive/cv-final-project/ablation_study/models_checklist.csv'\n","            df = pd.DataFrame(full_str_list, columns=['schedule', 'attention'])\n","            df.to_csv(path_str)"],"id":"uB6xCcy267IV"},{"cell_type":"markdown","metadata":{"id":"4PdwBUncBZHV"},"source":["## Loading Model\n","\n","We next load in each associated model combination of:\n","\n","```Python\n","    schedule_list = ['cosine_beta_schedule', 'linear_beta_schedule']\n","    attention_list = ['no_attention', 'self_attention']\n","```\n","\n","into a `dict` object named `model_dict`, where the keys are the associated `{schedule}-{attention}` and the values are complete `PyTorch` models."],"id":"4PdwBUncBZHV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKIOtNLX_gaV"},"outputs":[],"source":["model_channel_str = 'RGB_model_' if train_dataset.channels == 3 else 'gray_model_'"],"id":"OKIOtNLX_gaV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vruQe0mr8r-X"},"outputs":[],"source":["model_dict = {f'{schedule}-{attention}': torch.load(f'./drive/MyDrive/cv-final-project/ablation_study/ablation_trained/{schedule}/{attention}/' + model_channel_str + f'{train_dataset.image_size}px_{train_epochs}epoch_{schedule}_{attention}.pt') for schedule in schedule_list for attention in attention_list}"],"id":"vruQe0mr8r-X"},{"cell_type":"markdown","metadata":{"id":"a8337c82"},"source":["## Sampling (Inference)\n","\n","To sample from the model, we can just use our sample function defined above:\n","\n","(Note: given the ablation nature of this notebook, the produced images will not be perfect, however errors are assumed to have the same distribution across all models, and assuming all models having already converged).\n"],"id":"a8337c82"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAjv_EuvFIto"},"outputs":[],"source":["## Sample images\n","if SAMPLE_MODEL:\n","    sample_batch_size = 128\n","    model_dict_str = 'cosine_beta_schedule-self_attention'\n","\n","    samples, samples_img = sample_batch(model_dict[model_dict_str], batch_size=sample_batch_size, return_torch=False)"],"id":"XAjv_EuvFIto"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hnq1iboKHEW"},"outputs":[],"source":["model_dict[model_dict_str].show_images(samples_img, reverse_process=True) if SAMPLE_MODEL else None"],"id":"6hnq1iboKHEW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"_s-Al2lJ2c8T"},"outputs":[],"source":["# Show a random index\n","batch_idx = 43\n","model_dict[model_dict_str].show_images(samples_img[batch_idx]) if SAMPLE_MODEL else None"],"id":"_s-Al2lJ2c8T"},{"cell_type":"markdown","metadata":{"id":"0k4H1fmlKvzR"},"source":["We can also create a gif of the denoising process using all saved timesteps within the timestep dimension of the reverse process:"],"id":"0k4H1fmlKvzR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8vvj9xluTSv"},"outputs":[],"source":["reversed_samples = np.array([[model_dict[model_dict_str].reverse_transform(timestep_image) for timestep_image in sample] for sample in samples]) if SAMPLE_MODEL else None"],"id":"l8vvj9xluTSv"},{"cell_type":"code","execution_count":null,"metadata":{"id":"spE1I9aVNwzZ"},"outputs":[],"source":["if SAMPLE_MODEL:\n","    fig, ax = plt.subplots()\n","    images = []\n","    for t in range(model_dict[model_dict_str].timesteps):\n","        image = plt.imshow(reversed_samples[batch_idx][t], cmap='gray' if eval_dataset.channels == 1 else None, animated=True)\n","        images.append([image])\n","\n","    animate = animation.ArtistAnimation(fig, images, interval=10, repeat_delay=1000, blit=True)\n","    animate.save('diffusion.gif')"],"id":"spE1I9aVNwzZ"},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxhoKhAnc0Q9"},"outputs":[],"source":["if SAMPLE_MODEL:    \n","    video = animate.to_html5_video()\n","    HTML(video)"],"id":"GxhoKhAnc0Q9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8uxVbR0LSkc"},"outputs":[],"source":["def plot_reverse_fig(reversed_samples, title:str=None, nrows:int=32, ncols:int=32):\n","    fig, ax = plt.subplots(nrows, ncols, figsize=(34,34))\n","    plt.subplots_adjust(wspace=0, hspace=0)\n","\n","    for idx, img in enumerate(reversed_samples[:,-1]):\n","        plt.subplot(nrows, ncols, idx+1)\n","        plt.imshow(img)\n","        plt.axis('off')\n","\n","    plt.suptitle(title, y=0.89)"],"id":"Z8uxVbR0LSkc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZm_v88JrPX9"},"outputs":[],"source":["plot_reverse_fig(reversed_samples=reversed_samples, title='Generated Malignant Images') if SAMPLE_MODEL else None"],"id":"RZm_v88JrPX9"},{"cell_type":"markdown","metadata":{"id":"VDuCWc_ruclD"},"source":["# Evaluation\n","\n","Next, the dataset will be evaluated based on two metrics:\n","\n","1.    Maximum Likelihood\n","2.    Structural Similarity Score (SSIM)\n","\n","Where we obtain the score for each set of differently ablated models.\n","\n","## Likelihood\n","\n","The following function calculates the average negative log-likelihood (in `PyTorch` convention, `binary_cross_entropy_loss`) for all holdout, evaluation samples. Originally, likelihood estimation is presented as a method of evaluating GANs (as shown in [Eghbal-zadeh, Widmer](https://arxiv.org/pdf/1707.07530.pdf)). In effect, batches of model generated vs. holdout evaluation images are compared to provide a measure of how well generated images \"resemble\" evaluation images, where\n","\n","$$\\mathrm{BCE} = -\\sum_{s,b} \\left[y \\times \\mathrm{log}(p) + (1-y) \\times \\mathrm{log}(1-p) \\right]$$\n","\n","and\n","\n","1. $y$ is the evaluation label\n","2. $p$ is the predicted probability for the input class (if estimated result, sigmoid of the value)\n","3. $\\sum_{s, b}$ over all samples over all batches\n","\n","In rationale, the underlying distributions of the generated images are compared to that of the evaluation images, and finding the model which minimizes discernable differences selects for the best model, and we attempt to minimize the log likelihood (maximimizing the likelihood)."],"id":"VDuCWc_ruclD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOt5fouVwPHx"},"outputs":[],"source":["@torch.no_grad()\n","def compute_likelihood(diffusion_model, model_name, dataloader, device):\n","    total_likelihood = 0\n","    for batch_idx, data in enumerate(dataloader, 1):\n","        data = data.to(device)\n","        \n","        samples, samples_last_img = sample_batch(diffusion_model, batch_size=data.shape[0]) ## Sample from model   \n","        samples = np.array([reverse_transform(img) for img in samples_last_img]) ## Create a numpy array of reverse_transformed images using the last timestep for all batched\n","        samples = rearrange(samples, 'b h w c -> b c h w') ## Rearrange from [batch x height x width x channel] in generated images to [batch x channel x height x width] in dataset images\n","        samples = torch.Tensor(samples).to(device) ## Create torch.Tensor from samples\n","        \n","        samples = torch.sigmoid(samples) ## Apply sigmoid to get probabilities from samples\n","        total_likelihood += F.binary_cross_entropy(samples, data, reduction='mean').item()\n","        \n","        is_last_batch = True if len(dataloader) == batch_idx else False\n","\n","        image_num = batch_idx * dataloader.batch_size if not is_last_batch else len(dataloader.dataset)\n","        running_avg_likelihood = (total_likelihood / image_num) if not is_last_batch else (total_likelihood / len(dataloader.dataset))\n","        \n","        print(f'Model: {model_name} | Batch Index: {batch_idx}/{len(dataloader)} | Image: {image_num}/{len(dataloader.dataset)} | Running Average Likelihood: {running_avg_likelihood}')\n","\n","    average_likelihood = total_likelihood / len(dataloader.dataset)\n","    return average_likelihood"],"id":"aOt5fouVwPHx"},{"cell_type":"markdown","metadata":{"id":"ID_5pcu75Lqf"},"source":["## Structural Similarity Index\n","\n","As proposed in the 2004 paper [Image Quality Assessment: From Error Visibility to\n","Structural Similarity](https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf), Structural Similarity Index (or SSIM for short) compares generated images on three different features of *luminance*, *contrast*, and *structure*, where\n","\n","1. **luminance**: comparing between two images $X$ and $Y$, for pixel values $x$ and $y$ we determine the mean brightness for each using the equation: $\\mu_x = \\frac{1}{N} \\sum^{N}_{i=1}x_i$ (respectively the same for $y$ values), the luminance: $$l(\\mathbf{x}, \\mathbf{y}) = \\frac{x\\mu_x\\mu_y + C_1}{\\mu^2_x+\\mu^2_y + C_1}, C_1 = (LK_1)^2$$\n","\n","2. **contrast**: comparing between two images $X$ and $Y$, for pixel values $x$ and $y$ we determine the standard deviation brightness for each using the equation: $\\sigma_x = \\left( \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\mu_x)^2 \\right)^\\frac{1}{2}$ (respectively the same for $y$ values), the contrast: $$c(\\mathbf{x}, \\mathbf{y}) = \\frac{2\\sigma_x\\sigma_y + C_2}{\\sigma_x^2 + \\sigma_x^2 + C_2}, C_2 = (LK_2)^2$$\n","\n","    (where $L = 255$ or the dynamic range of RGB pixel values)\n","\n","3. **similarity**: comparing between two images $X$ and $Y$ for pixel values $x$ and $y$, the similarity is defined as $$s(\\mathbf{x}, \\mathbf{y}) = \\frac{\\sigma_{xy} + C_3}{\\sigma_x\\sigma_y + C_3}$$ where $\\sigma_{xy} = \\frac{1}{N-1}\\sum^{N}_{i=1}(x_i-\\mu_x)(y_i-\\mu_y)$ (and $C_3$ to be defined later on)\n","\n","Finally, $$ \\mathrm{SSIM}(\\mathbf{x},\\mathbf{y}) = [l(\\mathbf{x},\\mathbf{y})]^\\alpha \\cdot [c(\\mathbf{x},\\mathbf{y})]^\\beta \\cdot [s(\\mathbf{x},\\mathbf{y})]^\\gamma$$\n","\n","where $\\alpha>0, \\beta>0, \\gamma>0$.\n","\n","Simplifying by setting $\\alpha = \\beta = \\gamma = 1$ and $C_3 = C_2 / 2$, we obtain:\n","\n","$$\n","\\mathrm{SSIM}(\\mathbf{x},\\mathbf{y}) =  \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2+\\mu_y^2 + C_1)(\\sigma_x^2+\\sigma_y^2+C_2)}\n","$$\n","\n","The rationale being that while generated images can be different from evaluation set images, image classes of histopathological scans have enough discernable similarity between each other in terms of luminence, contrast, and structure that a discernable difference can be discerned between different architectures of machine learning models."],"id":"ID_5pcu75Lqf"},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEYoMDq0Sg0L"},"outputs":[],"source":["@torch.no_grad()\n","def compute_rgb_ssim(diffusion_model, model_name, dataloader, device):\n","    total_ssim = 0\n","    for batch_idx, data in enumerate(dataloader, 1):\n","        data = data.numpy() ## Convert batched images to numpy arrays\n","\n","        samples, samples_last_img = sample_batch(diffusion_model, batch_size=data.shape[0]) ## Sample from diffusion model   \n","        samples = np.array([reverse_transform(img) for img in samples_last_img]) ## Create a numpy array of reverse_transformed images using the last timestep for all batched\n","        samples = rearrange(samples, 'b h w c -> b c h w') ## Rearrange from [batch x height x width x channel] in generated images to [batch x channel x height x width] to be consistent with dataset images\n","\n","        ssim_value = ssim(im1=data, im2=samples, channel_axis=1, gaussian_weights=True, sigma=1.5, use_sample_covariance=False) ## Uses values found in Wang, et al.\n","\n","        total_ssim += ssim_value ## Add average calculated ssim per channel per batch to total_ssim\n","\n","        is_last_batch = True if len(dataloader) == batch_idx else False\n","\n","        image_num = batch_idx * dataloader.batch_size if not is_last_batch else len(dataloader.dataset)\n","        running_avg_ssim = (total_ssim / image_num) if not is_last_batch else (total_ssim / len(dataloader.dataset))\n","\n","        print(f'Model: {model_name} | Batch Index: {batch_idx}/{len(dataloader)} | Image: {image_num}/{len(dataloader.dataset)} | Running Average SSIM: {running_avg_ssim}')\n","\n","    ## Returns the average SSIM score for each channel per image\n","    return total_ssim / len(dataloader.dataset)"],"id":"cEYoMDq0Sg0L"},{"cell_type":"markdown","metadata":{"id":"W_0H3PpOvHgY"},"source":["## Evaluation Using Metrics\n","\n","Here, the dictionary of models created will be used to evaluate scores for each model and saved as a `.csv` file.\n","\n","The holdout `eval` images will be used to create a `DataLoader` object to compute the scores."],"id":"W_0H3PpOvHgY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww25257l_rQb"},"outputs":[],"source":["num_splits = 5\n","\n","image_size = eval_dataset.image_size\n","channels = eval_dataset.channels\n","eval_batch_size = 64\n","eval_dataset.return_PIL = False"],"id":"ww25257l_rQb"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IrwP7hKrP9pV"},"outputs":[],"source":["if EVAL_MODEL:\n","    for split in range(3, num_splits+1):\n","\n","        ablation_dict = {}\n","        for model_name, model in model_dict.items():\n","            ## Create a temporary dictionary for the current model\n","            tmp_dict = {}\n","\n","            ## Set device\n","            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","            ## Compute likelihood and add to tmp_dict\n","            average_likelihood = compute_likelihood(diffusion_model=model, model_name=model_name, dataloader=eval_loader, device=device)\n","            print('='*100)\n","            print(f'Added {model_name}\\'s average likelihood to ablation_dict')\n","            print('='*100)\n","            tmp_dict['average_likelihood'] = average_likelihood\n","\n","            ## Compute SSIM score and add to tmp_dict\n","            average_ssim = compute_rgb_ssim(diffusion_model=model, model_name=model_name, dataloader=eval_loader, device=device)\n","            print('='*100)\n","            print(f'Added {model_name}\\'s average SSIM to ablation_dict')\n","            print('='*100)\n","            tmp_dict['average_ssim'] = average_ssim\n","\n","            ## Set the ablation_dict key to be the current model's name (block_klass, loss_type) and add to main dictionary\n","            ablation_dict[model_name] = tmp_dict\n","        \n","            json_path = f'./drive/MyDrive/cv-final-project/ablation_study/ablation_eval_results/ablation_dict{split}.json'\n","\n","            with open(json_path, 'w') as json_file:\n","                json.dump(ablation_dict, json_file)"],"id":"IrwP7hKrP9pV"},{"cell_type":"markdown","metadata":{"id":"WQhSRNFDwoCI"},"source":["## Loading in Ablation Dict\n","\n","Now we load in each `.json` associated with an `ablation_dict` to view its' contents based on "],"id":"WQhSRNFDwoCI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBFUjYaUxX6G"},"outputs":[],"source":["for split in range(1, num_splits+1):\n","    json_path = f'./drive/MyDrive/cv-final-project/ablation_study/ablation_eval_results/ablation_dict{split}.json'\n","\n","    with open(json_path) as json_file:\n","        ablation_dict = json.load(json_file)\n","    \n","    ablation_dict = json.dumps(ablation_dict, indent=4)\n","    print('='*50)\n","    print(f'ablation_dict{split}')\n","    print('='*50)\n","    print(ablation_dict)"],"id":"WBFUjYaUxX6G"},{"cell_type":"markdown","metadata":{"id":"WNfIb7mACCR5"},"source":["## Creating Average `dict`\n","\n","Next, the average of all splits is take and the `average_ablation_dict` is created of the previously defined dicts"],"id":"WNfIb7mACCR5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxbE1u8WSJpA"},"outputs":[],"source":["def process_ablation_dicts(*files):\n","    # Initialize variables for accumulating results\n","    total_splits = len(files)\n","    aggregated_results = {}\n","\n","    # Loop through each .json file\n","    for file in files:\n","        with open(file, 'r') as f:\n","            data = json.load(f)\n","\n","        # Accumulate results for each key in the .json file\n","        for key, value in data.items():\n","\n","            if key not in aggregated_results:\n","                aggregated_results[key] = {\n","                    'average_likelihood': 0,\n","                    'average_ssim': 0\n","                }\n","\n","            aggregated_results[key]['average_likelihood'] += value['average_likelihood']\n","            aggregated_results[key]['average_ssim'] += value['average_ssim']\n","\n","    # Calculate per-split average\n","    for key in aggregated_results:\n","        aggregated_results[key]['average_likelihood'] /= total_splits\n","        aggregated_results[key]['average_ssim'] /= total_splits\n","\n","    return aggregated_results"],"id":"LxbE1u8WSJpA"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtwYS0gBb1Bd"},"outputs":[],"source":["files = glob.glob('./drive/MyDrive/cv-final-project/ablation_study/ablation_eval_results/*.json')\n","\n","split_averaged_ablation_dict = process_ablation_dicts(*files)\n","split_averaged_ablation_dict = json.dumps(split_averaged_ablation_dict, indent=4)\n","print(split_averaged_ablation_dict)"],"id":"QtwYS0gBb1Bd"},{"cell_type":"markdown","metadata":{"id":"tvalyGNwZR0N"},"source":["We end up choosing `cosine_beta_schedule-no_attention` as the architecture of choice, as the architecture with the lowest `average_likelihood` and highest `average_ssim`\n","\n","\n"],"id":"tvalyGNwZR0N"}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1U5Hxq8dPG55969J6ZuUAr2wx7HPmuEcr","timestamp":1679821339964},{"file_id":"1GYp9jxDwiPu3vfU6-EQnQT5CuNQ-YAqP","timestamp":1679054123992},{"file_id":"https://github.com/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb","timestamp":1677736406553}]},"gpuClass":"standard","jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}